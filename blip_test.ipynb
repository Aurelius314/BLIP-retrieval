{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70feed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd1fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example iamge and text\n",
    "sample_img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "sample_text = 'a woman sitting on the beach with a dog'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c48c45",
   "metadata": {},
   "source": [
    "# BLIP-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsh/miniconda3/envs/blip310/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/hsh/miniconda3/envs/blip310/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/hsh/miniconda3/envs/blip310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP1-large-for-zj 模型加载完成\n",
      "0.5116037\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipForImageTextRetrieval, AutoProcessor\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "blip_itm_large_coco_name = \"Salesforce/blip-itm-large-coco\"\n",
    "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-large-coco\", local_files_only=True)\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\", local_files_only=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"BLIP1-large-for-zj 模型加载完成\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text(text: str):\n",
    "    \"\"\"\n",
    "    编码文本，返回投影后的特征向量（与图像特征维度一致）\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(f\"text is required\")\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", truncation=True).to(device)\n",
    "    outputs = model.text_encoder(**inputs)\n",
    "    # 取 [CLS] token embedding\n",
    "    cls_emb = outputs.last_hidden_state[:, 0, :]  # shape [1, text_hidden_dim] 通常是 768\n",
    "    \n",
    "    # 关键修复：使用投影层将文本特征投影到与图像特征相同的维度\n",
    "    # HuggingFace 的 BlipForImageTextRetrieval 模型应该有 text_proj 层\n",
    "    # 参考官方实现 blip_image_text_matching.py 第 96 行：text_feat = F.normalize(self.text_proj(...), dim=-1)\n",
    "    if hasattr(model, 'text_proj'):\n",
    "        text_feat = model.text_proj(cls_emb)\n",
    "    else:\n",
    "        # 如果找不到投影层，打印模型结构帮助调试\n",
    "        print(\"模型属性:\", [attr for attr in dir(model) if not attr.startswith('_')])\n",
    "        raise AttributeError(\"模型没有找到 text_proj 层。请检查模型结构，可能需要使用不同的方法访问投影层。\")\n",
    "    \n",
    "    # L2 归一化\n",
    "    text_feat = torch.nn.functional.normalize(text_feat, dim=-1)\n",
    "    return text_feat[0].cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image(image_path: str):\n",
    "    \"\"\"\n",
    "    编码图像，返回投影后的特征向量（与文本特征维度一致）\n",
    "    \"\"\"\n",
    "    # 支持本地路径或URL\n",
    "    if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "        response = requests.get(image_path)\n",
    "        response.raise_for_status()\n",
    "        raw_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    elif os.path.exists(image_path):\n",
    "        raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid image path or URL: {image_path}\")\n",
    "\n",
    "    inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.vision_model(**inputs)\n",
    "    # 取 [CLS] token embedding（第一个token），对应官方实现中的 image_embeds[:, 0, :]\n",
    "    img_emb = outputs.last_hidden_state[:, 0, :]  # shape [1, vision_hidden_dim] 通常是 1024\n",
    "    \n",
    "    # 关键修复：使用投影层将图像特征投影到与文本特征相同的维度\n",
    "    # 参考官方实现 blip_image_text_matching.py 第 94 行：image_feat = F.normalize(self.vision_proj(...), dim=-1)\n",
    "    if hasattr(model, 'vision_proj'):\n",
    "        img_feat = model.vision_proj(img_emb)\n",
    "    else:\n",
    "        # 如果找不到投影层，打印模型结构帮助调试\n",
    "        print(\"模型属性:\", [attr for attr in dir(model) if not attr.startswith('_')])\n",
    "        raise AttributeError(\"模型没有找到 vision_proj 层。请检查模型结构，可能需要使用不同的方法访问投影层。\")\n",
    "    \n",
    "    # L2 归一化\n",
    "    img_feat = torch.nn.functional.normalize(img_feat, dim=-1)\n",
    "    return img_feat[0].cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def eval_retrieval(txt, img_path):\n",
    "    \"\"\"\n",
    "    计算文本和图像的余弦相似度\n",
    "    \"\"\"\n",
    "    text_emb = encode_text(txt)\n",
    "    img_emb = encode_image(img_path)\n",
    "\n",
    "    txt_vectors = np.array(text_emb, dtype=np.float32)\n",
    "    img_vectors = np.array(img_emb, dtype=np.float32)\n",
    "\n",
    "    # 特征已经归一化，直接计算点积即可得到余弦相似度\n",
    "    sim_score = np.dot(txt_vectors, img_vectors)\n",
    "    return sim_score\n",
    "\n",
    "\n",
    "sample_img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "sample_text = 'a woman sitting on the beach with a dog'\n",
    "if __name__ == '__main__':\n",
    "    sim = eval_retrieval(sample_text, sample_img_url)\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8267e",
   "metadata": {},
   "source": [
    "# BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blip2_feature_extractor 加载完成\n",
      "Similarity: 0.26567638\n"
     ]
    }
   ],
   "source": [
    "# simple check\n",
    "# reference: https://github.com/salesforce/LAVIS/blob/main/examples/blip2_feature_extraction.ipynb\n",
    "from lavis.common.registry import registry\n",
    "import os\n",
    "# lavis_root = \"/home/hsh/LAVIS/lavis\"\n",
    "# registry.register_path(\"library_root\", lavis_root)\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "# ========= 配置 =========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"blip2_feature_extractor\"\n",
    "model_type = \"pretrain\" \n",
    "\n",
    "input_path = \"/mnt/disk60T/dataset/Culture/Museum/Final_version/Final_Zhejiang.json\"\n",
    "output_path = \"/home/hsh/data/zj_blip2.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    name=model_name,\n",
    "    model_type=model_type,\n",
    "    is_eval=True,\n",
    "    device=device,\n",
    ")\n",
    "print(\"blip2_feature_extractor 加载完成\")\n",
    "\n",
    "# === text encoding ===\n",
    "@torch.no_grad()\n",
    "def encode_text(text: str):\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(f\"text is required\")\n",
    "    text_input = txt_processors[\"eval\"](text)\n",
    "    tokens = model.tokenizer(text_input, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    text_output = model.Qformer.bert(tokens.input_ids, attention_mask=tokens.attention_mask, return_dict=True,)\n",
    "    text_embeds = text_output.last_hidden_state # ([1, len, 768])\n",
    "    text_embed_cls = text_embeds[:, 0, :]  # [1, 768] - 使用 CLS token\n",
    "    # text_embed_mean = text_embeds.mean(dim=1)  # 所有 token 的平均值\n",
    "    return text_embed_cls.cpu().numpy().tolist()\n",
    "\n",
    "# === image encoding ===\n",
    "@torch.no_grad()\n",
    "def encode_image(image_path: str):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise ValueError(f\"image is required\")\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_input = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    img_features = model.extract_features({\"image\": image_input}, mode=\"image\")\n",
    "    img_embeds = img_features.image_embeds  # [1, 32, 768]\n",
    "    img_embeds_mean = img_embeds.mean(dim=1)    # [1, 768] - 平均所有 query tokens\n",
    "    return img_embeds_mean.cpu().numpy().tolist()\n",
    "\n",
    "def eval_retrieval(txt, img_path):\n",
    "    text_emb = encode_text(txt)\n",
    "    img_emb = encode_image(img_path)\n",
    "\n",
    "    txt_vectors = np.array(text_emb, dtype=np.float32)\n",
    "    img_vectors = np.array(img_emb, dtype=np.float32)\n",
    "\n",
    "    # squeeze 掉多余的维度 [1,768] -> [768]\n",
    "    txt_vectors = txt_vectors.squeeze()\n",
    "    img_vectors = img_vectors.squeeze()\n",
    "\n",
    "    # L2 normalization\n",
    "    txt_vectors /= np.linalg.norm(txt_vectors)\n",
    "    img_vectors /= np.linalg.norm(img_vectors)\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    sim_score = np.dot(txt_vectors, img_vectors)\n",
    "    return sim_score\n",
    "\n",
    "# sample text和image匹配\n",
    "# sample_img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "sample_img_url = '/home/hsh/BLIP_local/demo.jpg' \n",
    "sample_text = 'a woman sitting on the beach with a dog'\n",
    "if __name__ == '__main__':\n",
    "    sim = eval_retrieval(sample_text, sample_img_url)\n",
    "    print(\"Similarity:\", sim)\n",
    "\n",
    "# blip2_feature_extractor 加载完成\n",
    "# Similarity: 0.26567638   <---正常情况应该很高"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
